#统计分析
##自我报告：11个问卷和5个领域量表得分(data_al[, 1:16])
##认知任务：3个行为实验共14个指标(data_al[, 17:30])

#加载分析所用的包
```{r}
#if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  psych, lavaan, ggplot2, reshape2, caret,
  FactoMineR, factoextra, tidyverse, linkET,
  vegan, ggnewscale, RColorBrewer, EGAnet, xgboost,
  semPlot, randomForest, Metrics,corrplot, rsample
)
```

#数据标准化处理
```{r}
#计算各问卷总分和认知任务相关的z分数
data_al <- all_data_cleaned%>%
  select(matches("_al$"), -starts_with(c("phq", "gad", "SGPS", "swb","domain")),c("Ability","Attraction","Wealth","Social","Moral","obj_ses1",'fri_ses2',"ability_ALT_rt","moral_ALT_rt","ability_ALT_d","moral_ALT_d","ability_IAT","moral_IAT","ability_SRET_EW","moral_SRET_EW","ability_SRET_rt","moral_SRET_rt","ability_SRET_RJ1_d","moral_SRET_RJ1_d","ability_SRET_RJ2_d","moral_SRET_RJ2_d"),-"obj_ses1",-"fri_ses2")
data_al <- scale(data_al)
write.csv(data_al,"data_al_z.csv")
```

#问卷报告与认知任务相关性分析
```{r}
#自我报告+认知任务
cor_matrix <- cor(data_al, method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix, 
  method = "color", 
  type = "upper", 
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "相关系数热图"
)
```

#自我报告
```{r}
cor_matrix <- cor(data_al[,1:16], method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix, 
  method = "color", 
  type = "upper", 
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "自我报告相关系数热图"
)
```

#认知任务
```{r}
cor_matrix <- cor(data_al[, 17:30], method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix, 
  method = "color", 
  type = "upper", 
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "认知任务相关系数热图"
)
```

#保留一个变量时其他变量的预测程度（10折交叉验证）
```{r}
# 获取所有变量名
variables <- setdiff(names(data_z_cleaned), c("X.1", "X"))
# 初始化一个列表来保存模型结果
model_results <- list()
# 设置交叉验证的参数
control <- trainControl(method = "cv", number = 10) # 10折交叉验证
# 循环遍历每个变量
for(target_var in variables) {
# 排除当前目标变量，获取预测变量
  predictors <- setdiff(variables, target_var)
# 创建公式
  formula <- as.formula(paste(target_var, "~", paste(predictors, collapse = "+")))
# 训练模型，这里使用线性回归作为示例，你可以根据需要更换为其他模型
  model <- train(formula, data = data_z_cleaned, method = "lm", trControl = control)
# 保存模型结果
  model_results[[target_var]] <- model
}
# 输出每个模型的性能
for(target_var in names(model_results)) 
  cat("Model performance for", target_var, ":\n")
  print(model_results[[target_var]])
# 初始化一个数据框来保存所有模型的性能指标
performance_df <- data.frame(Target_Variable = character(),
                             RMSE = numeric(),
                             Rsquared = numeric(),
                             MAE = numeric(),
                             stringsAsFactors = FALSE)
# 提取每个模型的性能指标
for(target_var in names(model_results)) {
  # 提取性能指标
  results <- model_results[[target_var]]$results
  rmse <- ifelse(exists("RMSE", results), results$RMSE, NA)
  rsquared <- ifelse(exists("Rsquared", results), results$Rsquared, NA)
  mae <- ifelse(exists("MAE", results), results$MAE, NA)
# 将性能指标添加到数据框
  performance_df <- rbind(performance_df, data.frame(Target_Variable = target_var,
                                                     RMSE = rmse,
                                                     Rsquared = rsquared,
                                                     MAE = mae,
                                                     stringsAsFactors = FALSE))
}

# 将数据框写入 CSV 文件
write.csv(performance_df, "model_performance.csv", row.names = FALSE)
# 读取之前保存的性能指标数据框
performance_df <- read.csv("model_performance.csv")

# 创建RMSE的条形图
ggplot(performance_df, aes(x = reorder(Target_Variable, RMSE), y = RMSE)) +
  geom_bar(stat = "identity") +
  coord_flip() + # 翻转坐标轴，使得变量名更容易阅读
  labs(x = "Target Variable", y = "RMSE", title = "Root Mean Squared Error for Each Model") +
  theme_minimal()

# 创建R-squared的条形图
ggplot(performance_df, aes(x = reorder(Target_Variable, Rsquared), y = Rsquared)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Target Variable", y = "R-squared", title = "R-squared for Each Model") +
  theme_minimal()

# 创建MAE的条形图
ggplot(performance_df, aes(x = reorder(Target_Variable, MAE), y = MAE)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Target Variable", y = "MAE", title = "Mean Absolute Error for Each Model") +
  theme_minimal()
```

##对自我报告与认知任务分别降维(PCA)
#自我报告
```{r}
# 如未安装需要先安装所需的包
#install.packages("FactoMineR")
#install.packages("factoextra") # 用于结果的可视化

# 读取CSV文件
pca_q <- data_al[, 1:16]
#进行PCA降维处理
pca_q_result <- PCA(pca_q, graph = FALSE)
# 查看PCA结果
print(pca_q_result)
# 查看所有主成分
print(pca_q_result$eig)
# 生成碎石图
fviz_eig(pca_q_result, addlabels = TRUE, ylim = c(0, max(pca_q_result$eig) + 0.1 * max(pca_q_result$eig)))
#使用factoextra包进行结果的可视化
fviz_pca_ind(pca_q_result, col.ind = "cos2", 
              gradient.cols = c("blue", "red"), 
              repel = TRUE # Avoid text overlapping
)
pca_q_scores <- as.data.frame(pca_q_result$ind$coord)
write.csv(pca_q_scores, "PCA1.csv", row.names = FALSE)
```
#贝叶斯因子确定最佳因子数，EFA分析
```{r}
# 根据贝叶斯信息准则（BIC）确认最佳因子数（8）

fa_result <- list()
bic_values <- c()
for (i in 1:15) {  # 测试从 1 到 15 个因子
  fit <- fa(all_data_merged , nfactors = i, fm = "minres",  rotate = "varimax")
  fa_result[[i]] <- fit
  bic_values[i] <- fit$BIC
}
print(bic_values)
best_factors <- which.min(bic_values)
print(best_factors)  # 输出最佳因子数

# 基于最佳因子数进行探索性因子分析
efa_result_merged <- fa(all_data_merged, nfactors = best_factors, rotate = "oblimin", fm = "minres")
print(efa_result_merged)
# 可视化
fa.diagram(efa_result_merged, simple = T) # simple = FALSE会显示所有载荷，如果设置为TRUE则只显示显著的载荷
```


#聚类+网络热图
```{r}
#由于mantel检验无法对负值进行分析，因此在此步骤将计算z分数后的问卷总分进行平方处理
# 对data_al中的每个元素取平方
data_q_squared <- data_al[,1:16] ^ 2

# 重命名数据框为data_q_adjusted
data_q_adjusted <- data_q_squared
dist_q1 <- dist(data_q_adjusted, method = "euclidean")

# 对问卷做聚类（按列）
dist_col <- dist(t(data_q_adjusted), method = "euclidean")
hc <- hclust(dist_col, method = "ward.D2")
clusters <- cutree(hc, k = 3)

# 构建 spec_select
spec_select <- split(1:ncol(data_q_adjusted), clusters)
names(spec_select) <- paste0("Cluster_", 1:length(spec_select))

# 构建 Mantel 输入（分别是距离矩阵）
spec_dist <- dist(t(data_q_adjusted), method = "euclidean")  # 每个问卷是一个“样本”
env_dist <- dist(t(data_q_adjusted), method = "euclidean")

# 执行 Mantel 检验（每个问卷 vs 每个聚类）
mantel <- mantel_test(
  spec = data_q_adjusted,
  env = data_q_adjusted,
  spec_select = spec_select
) %>%
  mutate(
    rd = cut(r, breaks = c(-Inf, 0.2, 0.3, Inf),
             labels = c("< 0.2", "0.2 - 0.3", ">= 0.3")),
    pd = cut(p, breaks = c(-Inf, 0.005, 0.01, 0.05, Inf),
             labels = c("< 0.005", "0.005 - 0.01", "0.01 - 0.05", ">= 0.05"))
  )

# 计算相关矩阵
cor_matrix <- correlate(data_q_adjusted)

# 绘图
p <- qcorrplot(cor_matrix, type = "upper", diag = FALSE, grid_col = NA) +
  geom_point(shape = 21, size = 8, fill = NA, stroke = 0.35, color = "black") +
  geom_point(aes(size = abs(r), fill = r),
             shape = 21, stroke = 0.35, color = "black") +
  scale_size(range = c(1, 8), guide = "none") +
  new_scale("size") +
  geom_couple(data = mantel,
              aes(color = pd, size = rd),
              label.size = 3.88,
              label.family = "",
              label.fontface = 1,
              nudge_x = 0.2,
              curvature = nice_curvature(by = "from")) +
  scale_fill_gradientn(
    limits = c(-0.8, 0.8),
    breaks = seq(-0.8, 0.8, 0.4),
    colors = rev(brewer.pal(11, "Spectral"))
  ) +
  scale_size_manual(values = c(0.2, 0.7, 1.2)) +
  scale_color_manual(values = color_pal(4, alpha = 0.6)) +
  guides(
    size = guide_legend(title = "Mantel's r", order = 2, keyheight = unit(0.5, "cm")),
    colour = guide_legend(title = "Mantel's p", order = 1, keyheight = unit(0.5, "cm")),
    fill = guide_colorbar(title = "Pearson's r", keyheight = unit(2.2, "cm"), keywidth = unit(0.5, "cm"), order = 3)
  ) +
  theme(legend.box.spacing = unit(0, "pt"))
# 保存图像
ggsave(p, filename = "Cluster_Correlation_with_Mantel（1）.pdf", width = 9, height = 6)
```

#认知任务
```{r}
# 读取CSV文件
pca_task <- data_al[,17:30]
# 进行PCA降维处理
pca_task_result <- PCA(pca_task, graph = FALSE)
# 查看PCA结果
print(pca_task_result)
# 查看所有主成分
print(pca_task_result$eig)
# 生成碎石图
fviz_eig(pca_task_result, addlabels = TRUE, ylim = c(0, max(pca_task_result$eig) + 0.1 * max(pca_task_result$eig)))
#使用factoextra包进行结果的可视化
fviz_pca_ind(pca_task_result, col.ind = "cos2", 
              gradient.cols = c("blue", "red"), 
              repel = TRUE # Avoid text overlapping
)
pca_task_scores <- as.data.frame(pca_task_result$ind$coord)
write.csv(pca_task_scores, "PCA2.csv", row.names = FALSE)
```

#聚类+网络热图
```{r}
#由于mantel检验无法对负值进行分析，因此在此步骤将计算z分数后的问卷总分进行平方处理
# 对data_al中的每个元素取平方
data_q_squared <- data_al[,17:30] ^ 2

# 重命名数据框为data_q_adjusted
data_q_adjusted <- data_q_squared
dist_q1 <- dist(data_q_adjusted, method = "euclidean")

# 对问卷做聚类（按列）
dist_col <- dist(t(data_q_adjusted), method = "euclidean")
hc <- hclust(dist_col, method = "ward.D2")
clusters <- cutree(hc, k = 3)

# 构建 spec_select
spec_select <- split(1:ncol(data_q_adjusted), clusters)
names(spec_select) <- paste0("Cluster_", 1:length(spec_select))

# 构建 Mantel 输入（分别是距离矩阵）
spec_dist <- dist(t(data_q_adjusted), method = "euclidean")  # 每个问卷是一个“样本”
env_dist <- dist(t(data_q_adjusted), method = "euclidean")

# 执行 Mantel 检验（每个问卷 vs 每个聚类）
mantel <- mantel_test(
  spec = data_q_adjusted,
  env = data_q_adjusted,
  spec_select = spec_select
) %>%
  mutate(
    rd = cut(r, breaks = c(-Inf, 0.2, 0.3, Inf),
             labels = c("< 0.2", "0.2 - 0.3", ">= 0.3")),
    pd = cut(p, breaks = c(-Inf, 0.005, 0.01, 0.05, Inf),
             labels = c("< 0.005", "0.005 - 0.01", "0.01 - 0.05", ">= 0.05"))
  )

# 计算相关矩阵
cor_matrix <- correlate(data_q_adjusted)

# 绘图
p <- qcorrplot(cor_matrix, type = "upper", diag = FALSE, grid_col = NA) +
  geom_point(shape = 21, size = 8, fill = NA, stroke = 0.35, color = "black") +
  geom_point(aes(size = abs(r), fill = r),
             shape = 21, stroke = 0.35, color = "black") +
  scale_size(range = c(1, 8), guide = "none") +
  new_scale("size") +
  geom_couple(data = mantel,
              aes(color = pd, size = rd),
              label.size = 3.88,
              label.family = "",
              label.fontface = 1,
              nudge_x = 0.2,
              curvature = nice_curvature(by = "from")) +
  scale_fill_gradientn(
    limits = c(-0.8, 0.8),
    breaks = seq(-0.8, 0.8, 0.4),
    colors = rev(brewer.pal(11, "Spectral"))
  ) +
  scale_size_manual(values = c(0.2, 0.7, 1.2)) +
  scale_color_manual(values = color_pal(4, alpha = 0.6)) +
  guides(
    size = guide_legend(title = "Mantel's r", order = 2, keyheight = unit(0.5, "cm")),
    colour = guide_legend(title = "Mantel's p", order = 1, keyheight = unit(0.5, "cm")),
    fill = guide_colorbar(title = "Pearson's r", keyheight = unit(2.2, "cm"), keywidth = unit(0.5, "cm"), order = 3)
  ) +
  theme(legend.box.spacing = unit(0, "pt"))
# 保存图像
ggsave(p, filename = "Cluster_Correlation_with_Mantel（2）.pdf", width = 9, height = 6)
```

#对自我报告与认知任务的网络分析
```{r}
# Step 1: 数据准备
network_data1 <- data_al[, 1:16]  # 第一组数据
network_data2 <- data_al[, 17:30]  # 第二组数据
# Step 2: 网络分析
ega_model1 <- EGA(network_data1, model = "glasso")  # 第一组数据网络分析
ega_model2 <- EGA(network_data2, model = "glasso")  # 第二组数据网络分析
# 提取邻接矩阵
adj_matrix1 <- ega_model1$network  # 第一组网络邻接矩阵
adj_matrix2 <- ega_model2$network  # 第二组网络邻接矩阵
# Step 3: 合并网络
combined_matrix <- matrix(0, nrow = ncol(network_data1) + ncol(network_data2), 
                          ncol = ncol(network_data1) + ncol(network_data2))
rownames(combined_matrix) <- c(colnames(network_data1), colnames(network_data2))
colnames(combined_matrix) <- c(colnames(network_data1), colnames(network_data2))
# 填充邻接矩阵
combined_matrix[1:ncol(network_data1), 1:ncol(network_data1)] <- adj_matrix1
combined_matrix[(ncol(network_data1) + 1):nrow(combined_matrix), 
                (ncol(network_data1) + 1):ncol(combined_matrix)] <- adj_matrix2
# Step 4: 可视化
groups <- list(
  'Questionnaire' = 1:ncol(network_data1),
  'Task' = (ncol(network_data1) + 1):ncol(combined_matrix)
)

pdf('Network_UpDown.pdf')
qgraph(
  combined_matrix,
  groups = groups,
  layout = layout,        # 自定义布局：上下分布
  labels = FALSE,         # 不显示节点上的数字
  legend = TRUE,          # 显示图例
  label.cex = 0.8,        # 标签大小
  vsize = 5,              # 节点大小
  curveAll = TRUE,        # 显示所有边的曲率
  curve = 0.8,            # 边曲率
  edge.color = "black",   # 边的颜色
  color = c("#add3f4", "#fccb8e"),  # 第一组蓝色，第二组橙色
  theme = "colorblind",   # 色盲友好的主题
  title = "Network: Questionnaire and Task"
)
dev.off()
```

#对现实结果的预测：抑郁/焦虑/拖延/主观幸福感
##机器学习方法
###梯度提升机XGBoost：XGBoost迭代的机器学习技术，通过逐步添加新的模型（通常是弱学习器，如决策树）来修正现有模型的残差，从而提高整体模型的预测能力，是一种优化的分布式梯度提升决策树算法，在梯度提升框架下构建的，专门设计用于提高计算效率和模型性能，尤其在处理大规模数据集时表现优异。

#自我报告问卷
```{r}
#SGPS
# 准备数据
X <- as.matrix(pca_q_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$SGPS
dtrain <- xgb.DMatrix(data = X, label = y)
#选择最佳参数
#max_depths <- c(3, 6, 9)
#etas <- c(0.01, 0.1, 0.3)
#nrounds <- c(50, 100, 200)

# 训练模型：max_depth 决策树的最大深度；eta 学习率（学习速度）；nrounds 迭代次数；
xgb_model1 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
#查看哪些特征对预测影响最大
xgb.importance(feature_names = colnames(X), model = xgb_model1)
# 预测可视化
predictions_q_SGPS <- predict(xgb_model1, dtrain)
# 制作数据框用于绘图
results_q_SGPS <- data.frame(
  actual = y,
  predicted = predictions_q_SGPS
)
```

```{r}
#phq
X <- as.matrix(pca_q_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$phq
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model2 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model2)
# 预测可视化
predictions_q_phq <- predict(xgb_model2, dtrain)
# 制作数据框用于绘图
results_q_phq <- data.frame(
  actual = y,
  predicted = predictions_q_phq
)
```

```{r}
#gad
X <- as.matrix(pca_q_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$gad
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model3 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model3)
# 预测可视化
predictions_q_gad <- predict(xgb_model3, dtrain)
# 制作数据框用于绘图
results_q_gad <- data.frame(
  actual = y,
  predicted = predictions_q_gad
)
```

```{r}
#SWB
X <- as.matrix(pca_q_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$SWB
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model4 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model4)
# 预测可视化
predictions_q_SWB <- predict(xgb_model4, dtrain)
# 制作数据框用于绘图
results_q_SWB <- data.frame(
  actual = y,
  predicted = predictions_q_SWB
)
```

#合并四个小提琴图
```{r}
# 合并数据
results_q_SGPS$type <- "SGPS"
results_q_phq$type <- "PHQ"
results_q_gad$type <- "GAD"
results_q_SWB$type <- "SWB"
all_results_q <- bind_rows(results_q_SGPS, results_q_phq, results_q_gad, results_q_SWB)

# 绘制小提琴图
ggplot(all_results_q, aes(x = type, y = residuals)) +
  geom_violin(fill = "lightgoldenrod1", color = "black", alpha = 0.7) +
  geom_boxplot(width = 0.1, outlier.color = "red", outlier.size = 1, alpha = 0.5) +
  labs(title = "Distribution of Questionnaire Residuals for Different Metrics",
       x = "Metric",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()
```

```{r}
# 绘制散点图
ggplot(all_results_q, aes(x = actual, y = predicted, color = type)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +  # 理想线
  facet_wrap(~type) +  # 按类型分面
  labs(title = "Predicted vs Actual (Questionnaire)",
       x = "Actual Values",
       y = "Predicted Values") +
  theme_minimal() +
  theme(legend.position = "none")
```

#认知任务
```{r}
#SGPS
X <- as.matrix(pca_task_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$SGPS
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model5 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model5)
# 预测可视化
predictions_task_SGPS <- predict(xgb_model5, dtrain)
results_task_SGPS <- data.frame(
  actual = y,
  predicted = predictions_task_SGPS
)
results_task_SGPS$residuals <- results_task_SGPS$actual - results_task_SGPS$predicted
```

```{r}
#phq
X <- as.matrix(pca_task_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$phq
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model6 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model6)
# 预测可视化
predictions_task_phq <- predict(xgb_model6, dtrain)
results_task_phq <- data.frame(
  actual = y,
  predicted = predictions_task_phq
)
results_task_phq$residuals <- results_task_phq$actual - results_task_phq$predicted
```


```{r}
#gad
X <- as.matrix(pca_task_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$gad
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model7 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model7)
# 预测可视化
predictions_task_gad <- predict(xgb_model7, dtrain)
results_task_gad <- data.frame(
  actual = y,
  predicted = predictions_task_gad
)
results_task_gad$residuals <- results_task_gad$actual - results_task_gad$predicted
```

```{r}
#SWB
X <- as.matrix(pca_task_scores[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")])
y <- health_var$SWB
dtrain <- xgb.DMatrix(data = X, label = y)
xgb_model8 <- xgboost(data = dtrain, max_depth = 6, eta = 0.1, nrounds = 100, objective = "reg:squarederror")
xgb.importance(feature_names = colnames(X), model = xgb_model8)
# 预测可视化
predictions_task_SWB <- predict(xgb_model8, dtrain)
results_task_SWB <- data.frame(
  actual = y,
  predicted = predictions_task_SWB
)
results_task_SWB$residuals <- results_task_SWB$actual - results_task_SWB$predicted
```
#合并绘图
```{r}
# 合并数据
results_task_SGPS$type <- "SGPS"
results_task_phq$type <- "PHQ"
results_task_gad$type <- "GAD"
results_task_SWB$type <- "SWB"
all_results_task <- bind_rows(results_task_SGPS, results_task_phq, results_task_gad, results_task_SWB)

# 绘制小提琴图
ggplot(all_results_task, aes(x = type, y = residuals)) +
  geom_violin(fill = "orange", color = "black", alpha = 0.7) +
  geom_boxplot(width = 0.1, outlier.color = "red", outlier.size = 1, alpha = 0.5) +
  labs(title = "Distribution of Task Residuals for Different Metrics",
       x = "Metric",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()
```

```{r}
# 绘制散点图
ggplot(all_results_task, aes(x = actual, y = predicted, color = type)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +  # 理想线
  facet_wrap(~type) +  # 按类型分面
  labs(title = "Predicted vs Actual (TASK)",
       x = "Actual Values",
       y = "Predicted Values") +
  theme_minimal() +
  theme(legend.position = "none")
```

#Bifactor模型
```{r}
# 自动确定模型因子数量
fa_parallel <- fa.parallel(data_z, fa = "fa", fm = "minres", n.iter = 100)
nfactors <- fa_parallel$nfact  # 推荐因子数量
# 进行 EFA
efa_result <- fa(r = cor(data_z), nfactors = nfactors, fm = "minres", rotate = "oblimin")
# 打印因子分析结果，查看因子载荷
print(efa_result)
# 提取因子载荷矩阵
loadings <- as.data.frame(efa_result$loadings)
# 根据因子载荷动态生成 bifactor 模型公式
threshold <- 0.3  # 因子载荷的阈值
model_formula <- ""

for (factor_idx in 1:nfactors) {
  # 检查因子索引是否在范围内
  if (factor_idx <= ncol(loadings$x)) {
    # 提取当前因子相关的条目
    factor_items <- rownames(loadings$x)[abs(loadings$x[, factor_idx]) > threshold]
    factor_name <- paste0("F", factor_idx)
    
    # 为模型公式添加条目
    if (length(factor_items) > 0) {
      model_formula <- paste0(
        model_formula,
        factor_name, " =~ ", paste(factor_items, collapse = " + "), "\n"
      )
    }
  }
}

# 添加通用因子
general_factor_items <- rownames(loadings$x)
model_formula <- paste0(
  "I =~ ", paste(general_factor_items, collapse = " + "), "\n", 
  model_formula
)
# 打印生成的模型公式
cat(model_formula)

# 模型
model_formula <- '
  # 总体因子
  I =~ selfclarity_1 + selfclarity_2 + selfclarity_3 + selfclarity_4 + selfclarity_5 + selfclarity_6 + selfclarity_7 + selfclarity_8 + selfclarity_9 + selfclarity_10 + selfclarity_11 + selfclarity_12 + LOT_1 + LOT_2 + LOT_3 + LOT_4 + LOT_5 + LOT_6 + IPC_1 + IPC_2 + IPC_3 + IPC_4 + IPC_5 + IPC_6 + IPC_7 + IPC_8 + ses_1 + ses_2 + ses_3 + ses_4 + ses_5 + ses_6 + ses_7 + ses_8 + ses_9 + ses_10 + coreself_1 + coreself_2 + coreself_3 + coreself_4 + coreself_5 + coreself_6 + coreself_7 + coreself_8 + coreself_9 + coreself_10 + NPI1 + NPI2 + NPI3 + NPI4 + NPI5 + NPI6 + NPI7 + NPI8 + NPI9 + NPI10 + NPI11 + NPI12 + NPI13 + NPI14 + NPI15 + hsns_1 + hsns_2 + hsns_3 + hsns_4 + hsns_5 + hsns_6 + hsns_7 + hsns_8 + hsns_9 + hsns_10 + MorIden_1 + MorIden_2 + MorIden_3 + MorIden_4 + MorIden_5 + MorIden_6 + MorIden_7 + MorIden_8 + MorIden_9 + MorIden_10 + moralSeImag_1 + moralSeImag_2 + moralSeImag_3 + moralSeImag_4 + moralSeImag_5 + moralSeImag_6 + moralSeImag_7 + moralSeImag_8 + moralSeImag_9 + sde_1 + sde_2 + sde_3 + sde_4 + sde_5 + sde_6 + sde_7 + sde_8 + sde_9 + sde_10 + sde_11 + sde_12 + sde_13 + sde_14 + sde_15 + sde_16 + sde_17 + sde_18 + sde_19 + sde_20 + IM_1 + IM_2 + IM_3 + IM_4 + IM_5 + IM_6 + IM_7 + IM_8 + IM_9 + IM_10 + IM_11 + IM_12 + IM_13 + IM_14 + IM_15 + IM_16 + IM_17 + IM_18 + IM_19 + IM_20 + Ability + Attraction + Wealth + Social + Moral + ability_ALT_rt + moral_ALT_rt + ability_ALT_d + moral_ALT_d + ability_IAT + moral_IAT + ability_SRET_EW + moral_SRET_EW + ability_SRET_rt + moral_SRET_rt + ability_SRET_RJ1_d + moral_SRET_RJ1_d + ability_SRET_RJ2_d + moral_SRET_RJ2_d
  
  # 特定因子
F1 =~ LOT_3 + LOT_5 + IPC_6 + ses_1 + ses_2 + ses_3 + ses_4 + ses_5 + ses_6 + ses_7 + ses_8 + ses_9 + ses_10 + coreself_1 + coreself_2 + coreself_3 + coreself_4 + coreself_5 + coreself_6 + coreself_7 + coreself_8 + coreself_9 + coreself_10 + hsns_1 + sde_9 + sde_19 + Ability
F2 =~ selfclarity_1 + selfclarity_2 + selfclarity_3 + selfclarity_4 + selfclarity_5 + selfclarity_6 + selfclarity_7 + selfclarity_8 + selfclarity_9 + selfclarity_10 + selfclarity_11 + selfclarity_12 + LOT_2 + NPI7 + sde_5 + sde_7 + sde_20
F3 =~ moralSeImag_1 + moralSeImag_2 + moralSeImag_3 + moralSeImag_4 + moralSeImag_5 + moralSeImag_6 + moralSeImag_7 + moralSeImag_8 + moralSeImag_9 + Moral
F4 =~ IM_2 + IM_3 + IM_4 + IM_6 + IM_7 + IM_9 + IM_10 + IM_11 + IM_12 + IM_13 + IM_14 + IM_15 + IM_16 + IM_17 + IM_18 + IM_19 + IM_20
F5 =~ selfclarity_3 + selfclarity_10 + LOT_2 + coreself_2 + coreself_3 + hsns_3 + hsns_5 + hsns_6 + hsns_7 + hsns_8 + sde_2 + sde_6 + sde_10 + sde_12 + sde_14 + sde_18 + IM_1 + IM_3 + IM_5 + IM_7 + IM_15 + IM_19
F6 =~ NPI1 + NPI2 + NPI3 + NPI4 + NPI5 + NPI6 + NPI8 + NPI10 + NPI11 + NPI15 + Social
F7 =~ IPC_7 + MorIden_2 + MorIden_5 + MorIden_6 + MorIden_7 + MorIden_8 + MorIden_9 + MorIden_10
F8 =~ IPC_1 + IPC_2 + IPC_7 + sde_9 + sde_13 + ability_SRET_EW
F9 =~ hsns_9 + MorIden_3 + sde_1 + sde_3 + sde_11 + sde_15 + sde_17 + sde_19
F10 =~ ability_SRET_rt + moral_SRET_rt + ability_SRET_RJ2_d + moral_SRET_RJ2_d
F11 =~ ability_ALT_rt + moral_ALT_rt + ability_ALT_d + moral_ALT_d
F12 =~ MorIden_1 + MorIden_3 + IM_10

  # 因子之间的协方差
  #F1 ~~ F2 + F3
  #F4 ~~ F5 + F6
  #F7 ~~ F8
'
```

```{r}
# 拟合模型
fit = lavaan::sem(model_formula, 
                      data=data_z, 
                      std.lv = TRUE, 
                      fixed.x = F,
                      orthogonal = TRUE)
# 输出模型拟合结果和标准化估计
summary(fit , standardized = TRUE, fit.measures = TRUE)
# 提取因子得分
factor_scores <- lavPredict(fit, type = "ov", method = "EBM") # 使用expected a posteriori方法提取因子得分
# 查看因子得分
head(factor_scores)
```

```{r}
#模型拟合指标
fitMeasures(fit, c("p","cfi", "tli", "rmsea", "srmr","AIC"))
standardizedSolution(fit)
residuals(fit, type = "cor")
#CFI	比较拟合指数（Comparative Fit Index），评估模型拟合程度。	> 0.90（较好），> 0.95（优秀）。
#TLI	Tucker-Lewis 指数，类似于 CFI，用于评估拟合质量。	> 0.90（较好），> 0.95（优秀）。
#RMSEA	均方根误差近似（Root Mean Square Error of Approximation），反映模型误差大小。	< 0.08（较好），< 0.05（优秀）。
#SRMR	标准化残差均方根（Standardized Root Mean Residual），表示模型预测值与实际数据之间的差异。	< 0.08（较好）。
```

```{r}
# 提取因子得分
factor_scores <- predict(fit)  # `type = "factor.scores"` 不是 lavaan 支持的参数，直接用 predict 提取
head(factor_scores)  # 查看前几行因子得分

# 提取因子载荷矩阵
standardized_solution <- parameterEstimates(fit, standardized = TRUE)
factor_loadings <- subset(standardized_solution, op == "=~")  # 提取因子载荷
head(factor_loadings)  # 查看前几行因子载荷

# 可视化路径图
semPaths(fit, 
         what = "std",  # 标准化路径系数
         fade = FALSE,  # 不淡化路径
         layout = "tree",  # 树形布局
         style = "lisrel",  # LISREL 风格
         residuals = TRUE,  # 显示残差
         title = TRUE,  # 显示标题
         label.cex = 0.8,  # 调整路径标签大小
         edge.label.cex = 0.8,  # 调整路径系数大小
         nCharNodes = 0,  # 隐藏节点标签
         sizeMan = 8,  # 调整观测变量节点大小
         sizeLat = 10,  # 调整潜变量节点大小
         mar = c(5, 5, 5, 5))  # 调整边距
```


```{r}
model_formula <- cfa(model_formula, data_z,orthogonal=TRUE) # 进行CFA
summary(model_formula ,fit.measures="TRUE") # 显示总体结果
fitMeasures(model_formula ,fit.measures="all", baseline.model=NULL) # 显示所有拟合指数
standardizedSolution(model_formula) # 显示标准化的结果
reliability_results <- semTools::reliability(model_formula)# 计算AVE、CR（Composite Reliability）等值
semPaths(model_formula, bifactor = "G", 
         title=F, what = "std", residuals = FALSE,
         intercepts = FALSE, layout = "tree2",
         label.cex=1, edge.label.cex=0.1, font=2,  
         edge.color="black", fixedStyle = c("black",1),freeStyle = c("black",1),esize = 0.01,
         rotation=1)                                                                 # 画图,bifactor指定哪个因素为bifactor；rotation控制方向，取值为1/2/3/4
```



#随机森林
```{r}
# 将Bifactor分析的因子得分作为随机森林的自变量
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
#combined_data <- cbind(factor_scores, health_var)
# 检查合并后的数据框 
#head(combined_data)
#install.packages("rsample")  # 安装包
library(rsample)             # 加载包
library(randomForest)
#efa结果分析
set.seed(123)
rf_split <- initial_split(combined, prop = .7)# 70%训练集，30%测试集
rf_train <- training(rf_split)#训练集#352
rf_test  <- testing(rf_split)#测试集#151
```

```{r Tuning the parameters1}
#调参，选择maxnodes和ntree
# If training the model takes too long try setting up lower value of N
X_train_ = rf_train[1:352 , 1:8]
y_train_ = rf_train[1:352,10]#SGPS
rf.all.factor <- randomForest(x = X_train_, y = y_train_,type = regression, ntree = 100,importance = TRUE)
```

```{r}
#tune the model调参
set.seed(123)
tuneRF(rf_train[1:352 , 1:8],rf_train[1:352,10],stepFactor=3)
```

```{r}
#SGPS
X_test = rf_test[1:151 , 1:8]
y_test = rf_test[1:151,10]#SGPS
predictions <- predict(rf.all.factor, X_test)
result <- X_test
result['SGPS'] <- y_test
result['prediction']<-  predictions
head(result)  
ggplot(  ) + 
  geom_point(aes(x = X_test$MR1, y = y_test, color  = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = X_test$MR1 , y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "F1", y = "SGPS", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Predicted", "Real"), values = c("blue", "red")) 
plot(rf.all.factor)
```

```{r}
print(paste0('MAE: ' , mae(y_test,predictions) ))
## [1] "MAE: 742.401258870433"
print(paste0('MSE: ' ,caret::postResample(predictions , y_test)['RMSE']^2 ))
## [1] "MSE: 1717272.6547428"
print(paste0('R2: ' ,caret::postResample(predictions , y_test)['Rsquared'] ))
## [1] "R2: 0.894548902990278"

#[1] "MAE: 7.55968984547461"
#[1] "MSE: 77.4082373390361"
#[1] "R2: 0.0101757749592005"
```

```{r}
randomForest::importance(rf.all.factor)
```

```{r}
varImpPlot(rf.all.factor,sort = FALSE)
ImpData <- as.data.frame(randomForest::importance(rf.all.factor))
ImpData$Var.Names <- row.names(ImpData)
ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +  theme_light() +  coord_flip() +  theme(    legend.position="bottom",    panel.grid.major.y = element_blank(),    panel.border = element_blank(),    axis.ticks.y = element_blank()  )
```


```{r}
#SGPS
y_test_SGPS = rf_test[1:151,15] #gad
y_train_SGPS =rf_train[1:352,15] #gad

SGPS<- randomForest(x = X_train_, y = y_train_SGPS,type =regression, ntree = 100,importance = TRUE)
predictions_SGPS <- predict(SGPS, X_test)
result_SGPS <- X_test 
result_SGPS['gad'] <- y_test_SGPS
result_SGPS['prediction']<-predictions_SGPS
head(result_SGPS)   
```

```{r}
randomForest::importance(SGPS)
varImpPlot(SGPS,sort = FALSE)
```

```{r}
ImpData <- as.data.frame(randomForest::importance(SGPS))

 ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y= `%IncMSE`)) +  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend= `%IncMSE`), color="skyblue") +  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +  theme_light() +  coord_flip() +  theme(    legend.position="bottom",    panel.grid.major.y = element_blank(),    panel.border = element_blank(),    axis.ticks.y = element_blank()  )
```


```{r}
#gad
y_test_gad = rf_test[1:151,11] #gad
y_train_gad =rf_train[1:352,11] #gad

gad<- randomForest(x = X_train_, y = y_train_gad,type =regression, ntree = 100,importance = TRUE)
predictions_gad1 <- predict(gad, X_test)
result_gad1 <- X_test 
result_gad1['gad'] <- y_test_gad
result_gad1['prediction']<-predictions_gad1
head(result_gad1)   
mae <- mean(abs(y_test_gad - predictions_gad1))
mse <- mean((y_test_gad - predictions_gad1)^2)
r_squared <- cor(y_test_gad, predictions_gad1)^2
print(paste("MAE:", mae))
print(paste("MSE:", mse))
print(paste("R²:", r_squared))

```

```{r}
randomForest::importance(gad)
#varImpPlot(gad,sort = FALSE)
```

```{r}
ImpData <- as.data.frame(randomForest::importance(gad))

 ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y= `%IncMSE`)) +  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend= `%IncMSE`), color="skyblue") +  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +  theme_light() +  coord_flip() +  theme(    legend.position="bottom",    panel.grid.major.y = element_blank(),    panel.border = element_blank(),    axis.ticks.y = element_blank()  )
```

```{r}
# 根据目标变量（如 gad）与每个自变量的相关系数方向来估计正负
correlations <- cor(X_train_, y_train_gad)

# 获取变量重要性
importance_values <- randomForest::importance(gad)

# 添加相关系数方向
importance_with_direction <- as.data.frame(importance_values)
importance_with_direction$Direction <- ifelse(correlations > 0, "Positive", "Negative")

# 查看结果
print(importance_with_direction)

# 可视化（正负方向不同颜色）
library(ggplot2)
importance_with_direction$Variable <- rownames(importance_with_direction)
ggplot(importance_with_direction, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`, fill = Direction)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("Positive" = "blue", "Negative" = "red")) +
  labs(title = "Variable Importance with Direction",
       x = "Variables", y = "%IncMSE") +
  theme_minimal()

```



```{r}
#phq
y_test_phq = rf_test[1:151,12]#phq
y_train_phq =rf_train[1:352,12] #phq
phq<- randomForest(x = X_train_, y = y_train_phq,type =regression, ntree = 100,importance = TRUE)
predictions_phq <- predict(phq, X_test)
result_phq <- X_test
result_phq['phq'] <- y_test_phq
result_phq['prediction']<-predictions_phq
head(result_phq)   
# 计算性能指标
mse_phq <- mean((result_phq$prediction - result_phq$phq)^2)  # Mean Squared Error (MSE)
mae_phq <- mae(result_phq$phq, result_phq$prediction)         # Mean Absolute Error (MAE)
r_squared_phq <- cor(result_phq$prediction, result_phq$phq)^2 # R-squared (R²)

# 输出结果
cat("Mean Squared Error (MSE):", mse_phq, "\n")
cat("Mean Absolute Error (MAE):", mae_phq, "\n")
cat("R-squared (R²):", r_squared_phq, "\n")
```
```{r}
randomForest::importance(phq)
varImpPlot(phq,sort = FALSE)
```
```{r}
ImpData <- as.data.frame(randomForest::importance(phq))
 ImpData$Var.Names <- row.names(ImpData)
ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +  theme_light() +  coord_flip() +  theme(    legend.position="bottom",    panel.grid.major.y = element_blank(),    panel.border = element_blank(),    axis.ticks.y = element_blank()  )#y= `MeanDecreaseAccuracy`时无法运行
```
```{r}
# 计算目标变量（phq）与自变量的相关系数方向
correlations_phq <- cor(X_train_, y_train_phq)

# 获取变量重要性
importance_values_phq <- randomForest::importance(phq)

# 将变量重要性和相关性方向整合到一个数据框中
importance_with_direction_phq <- as.data.frame(importance_values_phq)
importance_with_direction_phq$Direction <- ifelse(correlations_phq > 0, "Positive", "Negative")

# 添加变量名称
importance_with_direction_phq$Variable <- rownames(importance_with_direction_phq)

# 查看结果
print(importance_with_direction_phq)

# 可视化：根据正负方向分配颜色
library(ggplot2)
ggplot(importance_with_direction_phq, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`, fill = Direction)) +
  geom_bar(stat = "identity", width = 0.6) +
  coord_flip() +
  scale_fill_manual(values = c("Positive" = "blue", "Negative" = "red")) +
  labs(title = "Variable Importance with Direction for PHQ",
       x = "Variables", y = "%IncMSE", fill = "Direction") +
  theme_minimal()

```




```{r}
#SWB
y_test_SWB = rf_test[1:151,13]#SWB
y_train_SWB =rf_train[1:352,13] #SWB

SWB <- randomForest(x = X_train_, y = y_train_SWB,type = regression, ntree = 100,importance = TRUE)
predictions_SWB <- predict(SWB,X_test)
result_SWB <- X_test
result_SWB['SWB'] <- y_test_SWB
result_SWB['prediction']<-  predictions_SWB
head(result_SWB)   
```
```{r}
print(paste0('MAE: ' , mae(y_test_SWB,predictions_SWB) ))
## [1] "MAE: 742.401258870433"
print(paste0('MSE: ' ,caret::postResample(predictions_SWB, y_test_SWB)['RMSE']^2 ))
## [1] "MSE: 1717272.6547428"
print(paste0('R2: ' ,caret::postResample(predictions_SWB, y_test_SWB)['Rsquared'] ))
## [1] "R2: 0.894548902990278"

#[1] "MAE: 6.25599558498896"
#[1] "MSE: 57.8779193005887"
#[1] "R2: 0.00597708772021511"
```

```{r}
randomForest::importance(SWB)
varImpPlot(SWB,sort = FALSE)
```

```{r}
ImpData <- as.data.frame(randomForest::importance(SWB))

 ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +  theme_light() +  coord_flip() +  theme(    legend.position="bottom",    panel.grid.major.y = element_blank(),    panel.border = element_blank(),    axis.ticks.y = element_blank()  )
```
```{r}
# 计算目标变量（SWB）与自变量的相关系数方向
correlations_SWB <- cor(X_train_, y_train_SWB)

# 获取变量重要性
importance_values_SWB <- randomForest::importance(SWB)

# 将变量重要性和相关性方向整合到一个数据框中
importance_with_direction_SWB <- as.data.frame(importance_values_SWB)
importance_with_direction_SWB$Direction <- ifelse(correlations_SWB > 0, "Positive", "Negative")

# 添加变量名称
importance_with_direction_SWB$Variable <- rownames(importance_with_direction_SWB)

# 查看结果
print(importance_with_direction_SWB)

# 可视化：根据正负方向分配颜色
library(ggplot2)
ggplot(importance_with_direction_SWB, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`, fill = Direction)) +
  geom_bar(stat = "identity", width = 0.6) +
  coord_flip() +
  scale_fill_manual(values = c("Positive" = "blue", "Negative" = "red")) +
  labs(title = "Variable Importance with Direction for SWB",
       x = "Variables", y = "%IncMSE", fill = "Direction") +
  theme_minimal()

```


```{r}
df <- combined
df <- df %>%
  pivot_longer(cols = starts_with("MR"), names_to = "Component", values_to = "Value")
  pivot_longer(
    cols = c("SGPS", "gad", "phq", "SWB"), # 指定要转换的列
    names_to = "variable",          # 新列名，用于存储原始列名
    values_to = "IncMSE"           # 新列名，用于存储原始列的值
  )

ggplot(df, aes(x = factor(Component) , y = IncMSE, shape = variable, color = variable)) +
  geom_point(size = 5, alpha = 0.4) +  # 添加散点图层
  #geom_line(aes(group = variable), linetype = "dashed",  alpha = 0.5) +  # 添加虚线图层
  labs(
    title = "",
    x = "主成分",
    y = "%IncMSE",
    shape = "因变量",
    color = "因变量"
  ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # 去除主网格线
    panel.grid.minor = element_blank(),  # 去除次网格线
    panel.border = element_rect(fill = NA, colour = "black", size = 0.5),  # 增加边框线
    axis.ticks = element_line(color = "black"),  # 设置刻度线的颜色
    axis.ticks.length = unit(0.2, "cm")  # 设置刻度线的长度
  )
```

```{r}
# 定义一个函数来计算重要性并返回一个数据框
calculate_importance <- function(model, var_name) {
  imp <- as.data.frame(randomForest::importance(model))
  imp$Var.Names <- row.names(imp)
  imp$Variable <- var_name
  return(imp)
}

# 训练随机森林模型
set.seed(123) # 为了可重复性
rf_SGPS <- randomForest(x = X_train_, y = y_train_SGPS, type = "regression", ntree = 100, importance = TRUE)
rf_gad <- randomForest(x = X_train_, y = y_train_gad, type = "regression", ntree = 100, importance = TRUE)
rf_phq <- randomForest(x = X_train_, y = y_train_phq, type = "regression", ntree = 100, importance = TRUE)
rf_SWB <- randomForest(x = X_train_, y = y_train_SWB, type = "regression", ntree = 100, importance = TRUE)

# 计算每个模型的重要性
imp_SGPS <- calculate_importance(rf_SGPS, "SGPS")
imp_gad <- calculate_importance(rf_gad, "gad")
imp_phq <- calculate_importance(rf_phq, "phq")
imp_SWB <- calculate_importance(rf_SWB, "SWB")

# 合并所有重要性数据
all_imp <- bind_rows(imp_SGPS, imp_gad, imp_phq, imp_SWB)

# 首先，根据%IncMSE对数据进行排序
all_imp_sorted <- all_imp %>%
  arrange(`%IncMSE`)

# 将Var.Names转换为有序因子，按照%IncMSE的排序顺序
all_imp_sorted$Var.Names <- factor(all_imp_sorted$Var.Names, levels = unique(all_imp_sorted$Var.Names))



all_imp_sorted$`%IncMSE` <- ifelse(all_imp_sorted$`%IncMSE` < 0, 0, all_imp_sorted$`%IncMSE`)

# 按照MSE大小对数据进行排序
all_imp_sorted <- all_imp_sorted %>% 
  arrange(desc(`%IncMSE`)) # 降序排列，如果需要升序排列，可以使用arrange(%IncMSE)

# 为Var.Names添加一个排序后的新列，用于在绘图时控制横轴的顺序
all_imp_sorted$Var.Names.Ordered <- factor(all_imp_sorted$Var.Names, levels = unique(all_imp_sorted$Var.Names))

# 使用ggplot绘制图形
ggplot(all_imp_sorted, aes(x = Var.Names, y = `%IncMSE`, color = Variable)) +
  geom_segment(aes(xend = Var.Names, yend = 0)) +
  geom_point(aes(shape = Variable), size = 3) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(
    x = "Principal Components",
    y = "Increase in MSE",
    color = "Variable",
    shape = "Variable"
  ) +
  scale_y_continuous(limits = c(0, max(all_imp_sorted$`%IncMSE`))) # 设置纵轴范围从0开始

```
#Lasso回归分析
```{r}
# 加载必要的包
library(glmnet)
library(ggplot2)
library(reshape2)


# 提取efa因子得分
efa_scores <- efa_result_merged$scores
# 将因子得分转换为数据框
efa_scores <- as.data.frame(efa_scores)
# 查看前几行因子得分
head(efa_scores)
combined <- cbind(efa_scores, health_var)


# 提取因子数据 (F1 ~ F12) 和健康指标 (第15 ~ 18列)
X <- as.matrix(combined[, 1:8])  # 因子数据
y <- as.matrix(combined[, 10:13])  # 健康指标

# 设置预测目标（以健康指标中的第1列为例，如需预测其他指标可更改列号）
target <- y[, 1]

# 标准化数据
X_scaled <- scale(X)
y_scaled <- scale(target)

# 拟合 LASSO 模型
set.seed(123)
lasso_model <- cv.glmnet(X_scaled, y_scaled, alpha = 1, nfolds = 10, standardize = TRUE)

# 提取最佳 lambda 值
best_lambda <- lasso_model$lambda.min

# 使用最佳 lambda 值重新拟合模型
final_model <- glmnet(X_scaled, y_scaled, alpha = 1, lambda = 0.001)

# 提取变量的回归系数
coefficients <- as.data.frame(as.matrix(coef(final_model)))
coefficients <- coefficients[-1, , drop = FALSE]  # 去掉截距项
colnames(coefficients) <- c("Coefficient")
coefficients$Factor <- rownames(coefficients)

# 按绝对值排序以查看贡献最大因子
coefficients <- coefficients[order(abs(coefficients$Coefficient), decreasing = TRUE), ]

# 可视化结果：展示因子预测力
ggplot(coefficients, aes(x = reorder(Factor, Coefficient), y = Coefficient, fill = Coefficient)) +
  geom_bar(stat = "identity", width = 0.6) +
  coord_flip() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  labs(title = "Predictive Power of Factors (LASSO Regression)",
       x = "Factors",
       y = "Coefficient") +
  theme_minimal()

# 输出最佳 lambda 和变量系数
print(paste("Best lambda:", best_lambda))
print(coefficients)

# 如需预测其他健康指标，将 `target` 替换为 y[, 2], y[, 3], 或 y[, 4]

```





```{r}
# 数据准备
coefficients$FactorID <- 1:nrow(coefficients)  # 为每个因子分配一个唯一的ID
coefficients$Angle <- 90 - 360 * (coefficients$FactorID - 0.5) / nrow(coefficients)  # 确定因子标签的角度
coefficients$hjust <- ifelse(coefficients$Angle < -90, 1, 0)  # 设置因子标签的对齐方式
coefficients$Angle <- ifelse(coefficients$Angle < -90, coefficients$Angle + 180, coefficients$Angle)  # 翻转角度以便更易阅读

# 自定义颜色映射，和示例中的颜色一致
custom_colors <- c(
  "#5AB4E5", "#D7E698", "#D99BBB", "#F9F299",
  "#F7A899", "#E87482", "#9B8191", "#8F888B"
)

# 绘制径向堆积柱状图
ggplot(coefficients, aes(x = as.factor(FactorID), y = abs(Coefficient), fill = Coefficient)) +
  # 绘制柱状图
  geom_bar(stat = "identity", width = 0.8) + 
  coord_polar(start = 0) +  # 转换为径向图
  # 设置颜色映射
  scale_fill_gradientn(colors = custom_colors) +
  # 添加因子标签
  geom_text(aes(label = Factor, y = abs(Coefficient) + 0.05, angle = Angle, hjust = hjust), size = 4, color = "black") +
  # 添加坐标轴和刻度线
  geom_hline(yintercept = c(0.025, 0.05,0.075, 0.10), color = "grey70", linetype = "dashed") +
  annotate("text", x = 0, y = c(0.025, 0.05,0.075, 0.10), label = c("25%", "50%", "75%", "100%"), color = "grey30", size = 4, vjust = -0.5) +
  # 调整标签和美化
  labs(title = "Predictive Power of Factors (Radial Chart)",
       x = "", y = "Predictive Power") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "right"
  )
```

```{r}
# 获取预测值
predictions <- predict(final_model, X_scaled)

# 计算指标
mse <- mean((y_scaled - predictions)^2)  # 均方误差
mae <- mean(abs(y_scaled - predictions))  # 平均绝对误差
r_squared <- 1 - sum((y_scaled - predictions)^2) / sum((y_scaled - mean(y_scaled))^2)  # R²

# 输出指标
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("R-squared (R²):", r_squared, "\n")

```



```{r}
# 读取 CSV 文件
all_data <- read.csv("all_data_cleaned.csv")

all_data_merged <- all_data %>%
  select(-ends_with("_al"), -matches("^(gad_|phq_|swb_|SGPS_)"),-c("gad","phq","obj_ses1","fri_ses2"),-X)
```
#对hsns问卷进行反向计分
```{r}
# 假设反向计分范围为 1-5
min_score <- 1
max_score <- 5

# 找到以 "hsns_" 开头的列
hsns_columns <- grep("^hsns_", names(all_data_merged), value = TRUE)

# 创建一个新的数据框用于存储反向计分后的数据
re_score <- all_data_merged

# 对这些列进行反向计分
for (col in hsns_columns) {
  re_score[[col]] <- max_score + min_score - all_data_merged[[col]]
}

cor_matrix <- cor(re_score, method = "pearson")  # 可选方法：pearson/spearman/kendall
corrplot(
  cor_matrix, 
  method = "color", 
  type = "upper", 
 tl.col = "black", 
 tl.srt = 45,
 diag = FALSE,
  col = colorRampPalette(c("blue", "white", "red"))(20),
  title = "反向计分相关系数热图"
)
```
#Bifactor
```{r}
# 运行平行分析以确定因子数
parallel_result <- fa.parallel(all_data_merged, fm = "minres", fa = "fa")

# 输出建议的因子数
best_nfactors <- parallel_result$nfact
print(paste("建议的因子数：", best_nfactors))
# 运行 EFA
efa_result <- fa(all_data_merged, nfactors = best_nfactors, rotate = "oblimin", fm = "minres")

# 输出因子载荷
print(efa_result)

# 检查因子解释的总方差比例
variance_explained <- efa_result$Vaccounted
print(variance_explained)
summary(fit, standardized = TRUE, fit.measures = TRUE)
bifactor_model <- '
  # 总体因子
  I =~ selfclarity_1 + selfclarity_2 + selfclarity_3 + selfclarity_4 + selfclarity_5 + selfclarity_6 + selfclarity_7 + selfclarity_8 + selfclarity_9 + selfclarity_10 + selfclarity_11 + selfclarity_12 + LOT_1 + LOT_2 + LOT_3 + LOT_4 + LOT_5 + LOT_6 + IPC_1 + IPC_2 + IPC_3 + IPC_4 + IPC_5 + IPC_6 + IPC_7 + IPC_8 + ses_1 + ses_2 + ses_3 + ses_4 + ses_5 + ses_6 + ses_7 + ses_8 + ses_9 + ses_10 + coreself_1 + coreself_2 + coreself_3 + coreself_4 + coreself_5 + coreself_6 + coreself_7 + coreself_8 + coreself_9 + coreself_10 + NPI1 + NPI2 + NPI3 + NPI4 + NPI5 + NPI6 + NPI7 + NPI8 + NPI9 + NPI10 + NPI11 + NPI12 + NPI13 + NPI14 + NPI15 + hsns_1 + hsns_2 + hsns_3 + hsns_4 + hsns_5 + hsns_6 + hsns_7 + hsns_8 + hsns_9 + hsns_10 + MorIden_1 + MorIden_2 + MorIden_3 + MorIden_4 + MorIden_5 + MorIden_6 + MorIden_7 + MorIden_8 + MorIden_9 + MorIden_10 + moralSeImag_1 + moralSeImag_2 + moralSeImag_3 + moralSeImag_4 + moralSeImag_5 + moralSeImag_6 + moralSeImag_7 + moralSeImag_8 + moralSeImag_9 + sde_1 + sde_2 + sde_3 + sde_4 + sde_5 + sde_6 + sde_7 + sde_8 + sde_9 + sde_10 + sde_11 + sde_12 + sde_13 + sde_14 + sde_15 + sde_16 + sde_17 + sde_18 + sde_19 + sde_20 + IM_1 + IM_2 + IM_3 + IM_4 + IM_5 + IM_6 + IM_7 + IM_8 + IM_9 + IM_10 + IM_11 + IM_12 + IM_13 + IM_14 + IM_15 + IM_16 + IM_17 + IM_18 + IM_19 + IM_20 + Ability + Attraction + Wealth + Social + Moral + ability_ALT_rt + moral_ALT_rt + ability_ALT_d + moral_ALT_d + ability_IAT + moral_IAT + ability_SRET_EW + moral_SRET_EW + ability_SRET_rt + moral_SRET_rt + ability_SRET_RJ1_d + moral_SRET_RJ1_d + ability_SRET_RJ2_d + moral_SRET_RJ2_d
  
  # 特定因子
F1 =~ LOT_3 + LOT_5 + IPC_6 + ses_1 + ses_2 + ses_3 + ses_4 + ses_5 + ses_6 + ses_7 + ses_8 + ses_9 + ses_10 + coreself_1 + coreself_2 + coreself_3 + coreself_4 + coreself_5 + coreself_6 + coreself_7 + coreself_8 + coreself_9 + coreself_10 + hsns_1 + sde_9 + sde_19 + Ability
F2 =~ selfclarity_1 + selfclarity_2 + selfclarity_3 + selfclarity_4 + selfclarity_5 + selfclarity_6 + selfclarity_7 + selfclarity_8 + selfclarity_9 + selfclarity_10 + selfclarity_11 + selfclarity_12 + LOT_2 + NPI7 + sde_5 + sde_7 + sde_20
F3 =~ moralSeImag_1 + moralSeImag_2 + moralSeImag_3 + moralSeImag_4 + moralSeImag_5 + moralSeImag_6 + moralSeImag_7 + moralSeImag_8 + moralSeImag_9 + Moral
F4 =~ IM_2 + IM_3 + IM_4 + IM_6 + IM_7 + IM_9 + IM_10 + IM_11 + IM_12 + IM_13 + IM_14 + IM_15 + IM_16 + IM_17 + IM_18 + IM_19 + IM_20
F5 =~ selfclarity_3 + selfclarity_10 + LOT_2 + coreself_2 + coreself_3 + hsns_3 + hsns_5 + hsns_6 + hsns_7 + hsns_8 + sde_2 + sde_6 + sde_10 + sde_12 + sde_14 + sde_18 + IM_1 + IM_3 + IM_5 + IM_7 + IM_15 + IM_19
F6 =~ NPI1 + NPI2 + NPI3 + NPI4 + NPI5 + NPI6 + NPI8 + NPI10 + NPI11 + NPI15 + Social
F7 =~ IPC_7 + MorIden_2 + MorIden_5 + MorIden_6 + MorIden_7 + MorIden_8 + MorIden_9 + MorIden_10
F8 =~ IPC_1 + IPC_2 + IPC_7 + sde_9 + sde_13 + ability_SRET_EW
F9 =~ hsns_9 + MorIden_3 + sde_1 + sde_3 + sde_11 + sde_15 + sde_17 + sde_19
F10 =~ ability_SRET_rt + moral_SRET_rt + ability_SRET_RJ2_d + moral_SRET_RJ2_d
F11 =~ ability_ALT_rt + moral_ALT_rt + ability_ALT_d + moral_ALT_d
F12 =~ MorIden_1 + MorIden_3 + IM_10

  # 因子之间的协方差
  #F1 ~~ F2 + F3
  #F4 ~~ F5 + F6
  #F7 ~~ F8
'
# 4. 拟合双因子模型
fit2 <- sem(bifactor_model, data = all_data_merged, std.lv = TRUE, orthogonal = TRUE)

# 5. 输出拟合结果
summary(fit2, standardized = TRUE, fit.measures = TRUE)

# 6. 检查模型拟合指标
fit_measures <- fitMeasures(fit2, c("rmsea", "cfi", "tli", "srmr"))
print(fit_measures)
```
```{r}
# 提取因子得分
factor_scores2 <- lavPredict(fit, type = "lv")

# 将因子得分转换为数据框
factor_scores_df <- as.data.frame(factor_scores2)

# 检查因子得分的前几行
head(factor_scores_df)

# 将因子得分与原始数据合并
combined_data2 <- cbind(factor_scores2 , health_var)
```

```{r}
# 提取因子数据 (F1 ~ F12) 和健康指标 (第15 ~ 18列)
X <- as.matrix(combined_data2[, 2:13])  # 因子数据
y <- as.matrix(combined_data2[, 15:18])  # 健康指标

# 设置预测目标（以健康指标中的第1列为例，如需预测其他指标可更改列号）
target <- y[, 4]

# 标准化数据
X_scaled <- scale(X)
y_scaled <- scale(target)

# 拟合 LASSO 模型
set.seed(123)
lasso_model <- cv.glmnet(X_scaled, y_scaled, alpha = 1, nfolds = 10, standardize = TRUE)

# 提取最佳 lambda 值
best_lambda <- lasso_model$lambda.min

# 使用最佳 lambda 值重新拟合模型
final_model <- glmnet(X_scaled, y_scaled, alpha = 1, lambda = 0.001)

# 提取变量的回归系数
coefficients <- as.data.frame(as.matrix(coef(final_model)))
coefficients <- coefficients[-1, , drop = FALSE]  # 去掉截距项
colnames(coefficients) <- c("Coefficient")
coefficients$Factor <- rownames(coefficients)

# 按绝对值排序以查看贡献最大因子
coefficients <- coefficients[order(abs(coefficients$Coefficient), decreasing = TRUE), ]
# 数据准备
coefficients$FactorID <- 1:nrow(coefficients)  # 为每个因子分配一个唯一的ID
coefficients$Angle <- 90 - 360 * (coefficients$FactorID - 0.5) / nrow(coefficients)  # 确定因子标签的角度
coefficients$hjust <- ifelse(coefficients$Angle < -90, 1, 0)  # 设置因子标签的对齐方式
coefficients$Angle <- ifelse(coefficients$Angle < -90, coefficients$Angle + 180, coefficients$Angle)  # 翻转角度以便更易阅读

# 自定义颜色映射，和示例中的颜色一致
custom_colors <- c(
  "#5AB4E5", "#D7E698", "#D99BBB", "#F9F299",
  "#F7A899", "#E87482", "#9B8191", "#8F888B"
)

# 绘制径向堆积柱状图
ggplot(coefficients, aes(x = as.factor(FactorID), y = abs(Coefficient), fill = Coefficient)) +
  # 绘制柱状图
  geom_bar(stat = "identity", width = 0.8) + 
  coord_polar(start = 0) +  # 转换为径向图
  # 设置颜色映射
  scale_fill_gradientn(colors = custom_colors) +
  # 添加因子标签
  geom_text(aes(label = Factor, y = abs(Coefficient) + 0.05, angle = Angle, hjust = hjust), size = 4, color = "black") +
  # 添加坐标轴和刻度线
  geom_hline(yintercept = c(0.025, 0.05,0.075, 0.10), color = "grey70", linetype = "dashed") +
  annotate("text", x = 0, y = c(0.025, 0.05,0.075, 0.10), label = c("25%", "50%", "75%", "100%"), color = "grey30", size = 4, vjust = -0.5) +
  # 调整标签和美化
  labs(title = "Predictive Power of Factors (Radial Chart)",
       x = "", y = "Predictive Power") +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16),
    legend.position = "right"
  )
```

